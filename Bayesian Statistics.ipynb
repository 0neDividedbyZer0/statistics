{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d6ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import t\n",
    "from scipy.stats import chi2\n",
    "import seaborn as sns\n",
    "import statsmodels.graphics.gofplots as sm\n",
    "import scipy.stats as sp\n",
    "import pandas as pd\n",
    "import math\n",
    "import sklearn as skl\n",
    "import random as rand\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce30e46",
   "metadata": {},
   "source": [
    "# Reflection (in progress)\n",
    " This is to write a bit on a reflection and on my opinion of being Bayesian vs. Frequentist. I myself am Frequentist. \n",
    " (not that Bayes is bad, but that pure Bayes always is bad, especially when the question doesn't make itself amenable to \n",
    " Bayes. Plus, Bayesian does get messed up with Bias and stuff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97585c27",
   "metadata": {},
   "source": [
    "# Explanation of the Bayesian Framework\n",
    "Mostly drawn from this link https://www.nature.com/articles/s43586-020-00001-2\n",
    "\n",
    "## Assumptions\n",
    "The big assumption of Bayesian statistics is that they treat the unknown quantity they wish to estimate as a random variable itself. This brings frequentists and Bayesians to blows as frequentists believe the unknown quantity is fixed, and that as time goes on, it converges towards a certain frequency.\n",
    "\n",
    "## Terms\n",
    "Bayesians don't stop here of course, they hurt frequentists further by introducing priors, beliefs, posteriors and all the like. They inject in subjectivity to their work. Here, I am less willing to argue with them. Remember, in frequentist statistics, we make assumptions too, of approximate normality in the population distribution, etc. The argument would then devolve into whoever's assumptions and subjectivity is more wrong, which is a horrible argument to have, so for civility, I would say this is fine. I do believe that in general, intuition on normality on the population distribution is usual, but the inclusion of beliefs and working with them is reasonable for Bayes.\n",
    "\n",
    "### Belief Distribution\n",
    "This is your current distribution of whatever you're trying to work with. It is termed belief since now you don't think there's a fixed quantity to be found, but this means you can adapt your belief. You will basically update the belief with Bayes Theorem.\n",
    "\n",
    "### Prior Distribution\n",
    "This is the subjective belief distribution towards parameters before you use your data. This is the main stickler of frequentists and Bayesians, as frequentists argue this is way too subjective. Furthermore, bad priors can result in terrible performance. Lastly, priors are hard to obtain. Very hard. Super duper ultra hard, which IS a good reason to use frequentist statistics. Methods such as empirical Bayes can estimate a possibly more correct prior. Some will just choose a reasonable or subjectively believed prior\n",
    "\n",
    "### Likelihood Distribution\n",
    "This is the distribution of the quantity you're looking for conditioned on the parameters.\n",
    "\n",
    "### Posterior Distribution \n",
    "This distribution is the updated prior using the collected data, it is usually more accurate. \n",
    "\n",
    "## Workflow\n",
    "The typical Bayesian workflow consists of\n",
    "three main steps: \n",
    "1. Capturing available knowledge about a given parameter in a statistical model via the prior distribution, which is typically determined before data collection; \n",
    "2. Determining the likelihood function using the information about the parameters available in the observed data; \n",
    "3. Combining both the prior distribution and the likelihood function using Bayes’ theorem in the form of the posterior distribution\n",
    "\n",
    "(From the article: Bayesian statistics and modelling Schoot, Depaoli, King, et al.)\n",
    "### Finding Priors\n",
    "Priors can come in many forms, but they have different levels of informativeness:\n",
    "1. Informative\n",
    "2. weakly informative\n",
    "3. diffuse\n",
    "Priors can range anywhere from complete to totally nonexistent certainty\n",
    "\n",
    "Prior elicitation is the method by which priors can be found and constructed\n",
    "\n",
    "Diffuse priors can have an adverse\n",
    "impact on parameter estimates via the posterior when\n",
    "sample sizes are small, especially in complex modelling\n",
    "situations involving meta-analytic models46, logistic\n",
    "regression models44 or mixture models47. In addition,\n",
    "improper priors are sometimes used with the intention of\n",
    "using them as diffuse priors. Although improper priors\n",
    "are common and can be implemented with relative ease\n",
    "within various Bayesian programs, it is important to note\n",
    "that improper priors can lead to improper posteriors. We\n",
    "mention this caveat here because obtaining an improper\n",
    "posterior can impact the degree to which results can be\n",
    "substantively interpreted. Overall, we note that a diffuse\n",
    "prior can be used as a placeholder before analyses of\n",
    "the same or subsequent data are conducted with more\n",
    "informative priors.\n",
    "\n",
    " Overall, there is no right or wrong\n",
    "prior setting. Many times, diffuse priors can produce\n",
    "results that are aligned with the likelihood, whereas\n",
    "sometimes inaccurate or biased results can be obtained\n",
    "with relatively flat priors47. Likewise, an informative\n",
    "prior that does not overlap well with the likelihood can\n",
    "shift the posterior away from the likelihood, indicating that inferences will be aligned more with the prior\n",
    "than the likelihood. Regardless of the informativeness\n",
    "of the prior, it is always important to conduct a prior\n",
    "sensitivity analysis to fully understand the influence\n",
    "that the prior settings have on posterior estimates48,49.\n",
    "When the sample size is small, Bayesian estimation\n",
    "with mildly informative priors is often used but the prior specification might have a huge effect on the\n",
    "posterior results.\n",
    "\n",
    "When priors do not conform with the likelihood, this\n",
    "is not necessarily evidence that the prior is not appropriate. It may be that the likelihood is at fault owing to\n",
    "a mis-specified model or biased data. The difference\n",
    "between the prior and the likelihood may also be reflective of variation that is not captured by the prior or likelihood alone. These issues can be identified through a\n",
    "sensitivity analysis of the likelihood, by examining different forms of the model, for example, to assess how the\n",
    "priors and the likelihood align\n",
    "\n",
    "Because inference based on a Bayesian analysis is subject\n",
    "to the ‘correctness’ of the prior, it is of importance to carefully check whether the specified model can be considered\n",
    "to be generating the actual data. This is partly done\n",
    "by means of a process known as prior predictive checking.\n",
    "\n",
    "Even in the\n",
    "case of a valid prior elicitation procedure, it is extremely\n",
    "important to understand the exact probabilistic specification of the priors. This is especially true for complex\n",
    "models with smaller sample sizes9\n",
    ". Because smaller sample sizes usually convey less information, priors, in comparison, will exhibit a strong influence on the posteriors.\n",
    "Prior predictive checking is an exercise to improve the\n",
    "understanding of the implications of the specified priors\n",
    "on possible observations. It is not a method for changing\n",
    "the original prior, unless this prior explicitly generates\n",
    "incorrect data.\n",
    "\n",
    "The prior predictive distribution is a distribution of all possible samples that could\n",
    "occur if the model is true. In theory, a ‘correct’ prior\n",
    "provides a prior predictive distribution similar to the\n",
    "true data-generating distribution54. Prior predictive\n",
    "checking compares the observed data, or statistics of the\n",
    "observed data, with the prior predictive distribution, or\n",
    "statistics of the predictive distribution, and checks their\n",
    "compatibility. For instance, values are drawn from\n",
    "the prior distributions. Using kernel density estimation,\n",
    "a non-parametric smoothing approach used to approximate a probability density function, the original sample and the samples from the predictive distribution\n",
    "can be compared. Alternatively, the compatibility can\n",
    "be summarized by a prior predictive p-value, describing\n",
    "how far the characteristics of the observed data lie in\n",
    "the tails of the reference prior predictive distribution\n",
    "\n",
    "Young and Pettit argued that measures based on\n",
    "the tail area of the prior predictive distribution, such\n",
    "as the approaches of Box and Evans and Moshonov, do\n",
    "not favour the more precise prior in cases where two\n",
    "priors are both specified at the correct value. Instead,\n",
    "they propose using a Bayes factor to compare two priors. The Bayes factor would favour the more precise\n",
    "prior. These three approaches leave the determination\n",
    "of prior–data conflict subjective, depending on an arbitrary cut-off value. The data agreement criterion tries\n",
    "to resolve the prior–data conflict determination issue by\n",
    "introducing a clear classification, removing the subjective element of this decision66. This is done at the expense\n",
    "of selecting an arbitrary divergence-based criterion\n",
    "\n",
    "An alternative criterion has been developed that computes whether the distance between the prior and the\n",
    "data is unexpected. For a comparison of both criteria, we\n",
    "direct the reader to Lek and van de Schoot\n",
    "\n",
    "\n",
    "#### Standard Bayesian Methods\n",
    "They set the prior before data collection. One way is expert consultation to find the hyperparameters, consulting either one or a number of experts. Using previous publications or meta-analyses can also work, or some combination thereof. \n",
    "\n",
    "#### Empirical Bayesian Methods\n",
    "They set the prior after data collection, estimating it from the collected data itself. These can lead to double-dipping/circular analysis. Maximum likelihood or use of sample statistics can do the job. It usually involves employing some evolving estimation of the prior that will be updated.\n",
    "\n",
    "### Finding Likelihood\n",
    "In both inference paradigms, its role is to\n",
    "quantify the strength of support the observed data lends\n",
    "to possible value(s) for the unknown parameter(s).\n",
    "\n",
    "In some cases, specifying a likelihood function can\n",
    "be very straightforward. However, in practice, the\n",
    "underlying data-generating model is not always known.\n",
    "Researchers often naively choose a certain data-generating\n",
    "model out of habit or because they cannot easily change\n",
    "it in the software. Although based on background\n",
    "knowledge, the choice of the statistical data-generating\n",
    "model is subjective and should therefore be well understood, clearly documented and available to the reader.\n",
    "Robustness checks should be performed on the selected\n",
    "likelihood function to verify its influence on the posterior\n",
    "estimates. Although most research on Bayesian robustness focuses on the sensitivity of the posterior results to\n",
    "the specification of the prior, a few contributions have\n",
    "focused on the sensitivity of the posterior results to the\n",
    "specification of the likelihood function\n",
    "\n",
    "\n",
    "### What happens after the Posterior?\n",
    "After specifying the prior and the likelihood, and collecting the data, the posterior distribution can be obtained.\n",
    "Here, we explain how a model can be fitted to data to\n",
    "obtain a posterior distribution, how to select variables\n",
    "and why posterior predictive checking is needed. Model\n",
    "building is an iterative process; any Bayesian model can\n",
    "be viewed as a placeholder that can be improved in\n",
    "response to new data or lack of fit to existing data, or\n",
    "simply through a process of model refinement\n",
    "\n",
    "(essentially, the posterior distribution is the sampling distribution for Bayesians)\n",
    "\n",
    "#### Model Fitting\n",
    "Within the\n",
    "Bayesian framework for model fitting, probabilities are\n",
    "assigned to the model parameters, describing the associated uncertainties. In Bayesian statistics, the focus is\n",
    "on estimating the entire posterior distribution of the\n",
    "model parameters. This posterior distribution is often\n",
    "summarized with associated point estimates, such as the\n",
    "posterior mean or median, and a credible interval. Direct\n",
    "inference on the posterior distribution is typically not\n",
    "possible, as the mathematical equation describing the\n",
    "posterior distribution is usually both very complicated\n",
    "and high-dimensional, with the number of dimensions\n",
    "equal to the number of parameters. The expression\n",
    "for the posterior distribution is typically only known\n",
    "up to a constant of proportionality, a constant term in\n",
    "the posterior distribution that is not a function of the\n",
    "parameters and, in general, cannot be explicitly calculated. In particular, the denominator of the expression\n",
    "for the posterior distribution is a function of only the\n",
    "data, where this function is not available in closed form\n",
    "but expressible only as an analytically intractable integral. This means that we cannot evaluate the posterior\n",
    "distribution exactly, and so cannot calculate, for example, associated summary statistics of interest directly.\n",
    "Further, the high dimensionality exacerbates these problems, so that calculating the marginal posterior distribution\n",
    "may also not be tractable, and expressible only in integral\n",
    "form. **We note that this intractability of the posterior distribution was the primary practical reason why Bayesian\n",
    "statistics was discarded by many scientists in favour of\n",
    "frequentist statistics.**\n",
    "\n",
    "The seminal article by Gelfand and Smith described how Markov chain Monte Carlo\n",
    "(MCMC), a technique for sampling from a probability\n",
    "distribution, can be used to fit models to data within\n",
    "the Bayesian paradigm. In particular, the MCMC algorithm only requires the probability distribution of interest to be specified up to a constant of proportionality and\n",
    "is scalable to high dimensions.\n",
    "\n",
    "MCMC is able to indirectly\n",
    "obtain inference on the posterior distribution using\n",
    "computer simulations\n",
    "\n",
    "Here, we focus on MCMC for posterior inference.\n",
    "MCMC combines two concepts: obtaining a set of\n",
    "parameter values from the posterior distribution using\n",
    "the Markov chain; and obtaining a distributional estimate\n",
    "of the posterior and associated statistics with sampled\n",
    "parameters using Monte Carlo integration. Although\n",
    "MCMC is the most common class of algorithm used\n",
    "in Bayesian analyses, there are other model-fitting\n",
    "algorithms. Other available estimators can be\n",
    "found elsewhere\n",
    "\n",
    "Metropolis-Hastings is of course useful, Hamiltonian Monte Carlo, Gibbs sampler, Particle MCMC, Evolutionary Monte Carlo\n",
    "\n",
    "The Gibbs sampler, the\n",
    "Metropolis–Hastings algorithm and Hamiltonian\n",
    "Monte Carlo are standard approaches for defining the\n",
    "transition kernel so that the corresponding stationary\n",
    "distribution is the correct posterior distribution. (MCMC is based on building the correct posterior as the resulting stationary distribution)\n",
    "\n",
    "#### Assessments\n",
    "Trace plots and R statistic are the standard ways of evaluating the MCMC method. The big debate over MCMC is the issue of convergence. \n",
    "\n",
    "PyMC3 is a Bayesian probability library for Python\n",
    "\n",
    "Once a posterior distribution for a particular model\n",
    "is obtained, it can be used to simulate new data conditional on this distribution that might be helpful to\n",
    "assess whether the model provides valid predictions so\n",
    "that these can be used for extrapolating to future events.\n",
    "Those simulations can be used for several purposes.\n",
    "They can be used to check whether the simulated data\n",
    "from the model resemble the observed data by comparing kernel density estimates of the observed data with\n",
    "density estimates for the simulated data57. A more formal\n",
    "posterior predictive checking approach can be taken to\n",
    "evaluate whether the model can be considered a good\n",
    "fit with the data-generating mechanism\n",
    "\n",
    "#### Variational Bayes\n",
    "An alternative approach\n",
    "is to produce functional approximations of the posterior using techniques including variational inference or\n",
    "expectation propagation. Here, we describe variational\n",
    "inference, also known as variational methods or variational Bayes, owing to its popularity and prevalence of\n",
    "use in machine learning.\n",
    "\n",
    "Variational inference begins by constructing an\n",
    "approximating distribution to estimate the desired —\n",
    "but intractable — posterior distribution. Typically, the\n",
    "approximating distribution chosen is from a family of\n",
    "standard probability distributions, for example multivariate normal distributions, and further assumes that some\n",
    "of the dependencies between the variables in our model\n",
    "are broken to make subsequent computations tractable. \n",
    "\n",
    "The approximating distribution will be specified up to a set of variational parameters\n",
    "that we optimize to find the best posterior approximation\n",
    "by minimizing the Kullback–Leibler divergence from the\n",
    "true posterior. As a consequence, variational inference\n",
    "reframes Bayesian inference problems as optimization\n",
    "problems rather than as sampling problems, allowing them to be solved using numerical optimization.\n",
    "When combined with subsampling-based optimization\n",
    "techniques such as stochastic gradient descent, variational inference makes approximate Bayesian inference\n",
    "possible for complex large-scale problems\n",
    "\n",
    "Now we should do a bit of examples to better understand Bayesian inference and modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afeb7aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayes Estimation, inference (confidence intervals + significance testing + Bayes factor), \n",
    "# Bayes regression/modeling (hierarchical models and stuff)\n",
    "# Bayes filtering <3. I do like using Bayesian filtering, this is a problem I enjoy for Bayes.\n",
    "# Nonparametric Bayes + Robust Bayesian stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dd06cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scholarsarchive.byu.edu/cgi/viewcontent.cgi?article=1277&context=facpub\n",
    "# Bayesian Average estimates the mean.\n",
    "# https://fulmicoton.com/posts/bayesian_rating/\n",
    "\n",
    "#Bayesian Statistics https://en.wikipedia.org/wiki/Bayesian_statistics, https://amstat.tandfonline.com/doi/abs/10.1080/00031305.1986.10475342#.YPYDcZhKjIU, https://projecteuclid.org/journals/bayesian-analysis/volume-3/issue-3/Objections-to-Bayesian-statistics/10.1214/08-BA318.full\n",
    "# Bayesian hierarchical modeling, Bayesan Network,\n",
    "# https://www.nature.com/articles/s43586-020-00001-2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
